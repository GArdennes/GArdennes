{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOC29JJje8sQAZbGrSHrzeJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GArdennes/GArdennes/blob/main/Decision_Trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To study binary regression trees we consider an example from Ross Quinlan's datasets. The idea is to predict whether to play tennis based on weather information."
      ],
      "metadata": {
        "id": "lZckBzbzef7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "filename = 'https://github.com/lmassaron/datasets/'\n",
        "filename += 'releases/download/1.0/tennis.feather'\n",
        "tennis = pd.read_feather(filename)"
      ],
      "metadata": {
        "id": "51hYwhOPeYMS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the dataset generated, we need to manipulate the data to separate the features from the target. The features are classification values."
      ],
      "metadata": {
        "id": "giJLjFw_fBqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tennis.keys()) #tells us what the header of the file says"
      ],
      "metadata": {
        "id": "KBtFM-wWfPMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tennis[['outlook','temperature','humidity','wind']]\n",
        "X = pd.get_dummies(X)\n",
        "y = tennis.play"
      ],
      "metadata": {
        "id": "vBKDMsMegqVC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now import the DecisionTree algorithm to fit all the data and observe the rules to predict the target"
      ],
      "metadata": {
        "id": "JB4xZBzqhvtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(X,y)"
      ],
      "metadata": {
        "id": "PEIKLs3zhPLw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we visualize the data"
      ],
      "metadata": {
        "id": "xCa1HOx2iUua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dtreeviz\n"
      ],
      "metadata": {
        "id": "S-39RoGzifkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install graphviz"
      ],
      "metadata": {
        "id": "zHn9ZOfxkPi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dtreeviz.trees import dtreeviz\n",
        "viz = dtreeviz(dt, X, y, target_name='play_tennis',feature_names = X.columns, class_names = [\"No\",\"Yes\"], scale =2.0)"
      ],
      "metadata": {
        "id": "BROVqBUejQy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "viz"
      ],
      "metadata": {
        "id": "mkarKTuNkhdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bars represent the distribution of the target at each node. With the first node, the root, with the feature outlook overcast the target is divided. If the weather outlook is overcast(probability greater than 0.5), yes you can play, creating a leaf. If the weather outlook is not overcast(probability less than 0.5) we consider the next node since the probability less than 0.5 contains both yes and no. The goal is to divide the dataset precisely at the features which make play possible and the features which don't. The next node considers the feature humidity and distributes the target(play) according to that. The iteration continues until the last nodes are reached. The target values contain only one type in the leaves. In the end it should be obvious to us what rule determines if playing tennis would be possible in the face of these features."
      ],
      "metadata": {
        "id": "upHlJT3rlGMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rule would be..."
      ],
      "metadata": {
        "id": "11RdXYpiqFEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "if outlook overcast > ... then play tennis\n",
        "else\n",
        "if outlook overcast <= ... and humidity > ... and wind strong <= ... then play tennis\n",
        "else\n",
        "if outlook overcast <= ... and humidity > ... and wind strong > ... and outlook rain is <= ... then play tennis\n",
        "else\n",
        "if outlook overcast <= ... and humidity <= ... and outlook sunny <= ... and wind weak > ... then play tennis"
      ],
      "metadata": {
        "id": "BG_0i9gPoM0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To take Regression trees further, we consider another dataset with a lot of noise(outliers) that is the RMS titanic dataset. With this dataset, we consider splitting the data into 75% for training and 25% for testing to evaluate our model fit."
      ],
      "metadata": {
        "id": "lKLSJI-Sshqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'https://github.com/lmassaron/datasets/'\n",
        "filename += 'releases/download/1.0/titanic.feather'\n",
        "titanic = pd.read_feather(filename)"
      ],
      "metadata": {
        "id": "3EvW08p5qDHu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(titanic.keys()) #tells us what the header of the file says"
      ],
      "metadata": {
        "id": "DnNoT39FtUKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = titanic.iloc[:,:-1]\n",
        "y = titanic.iloc[:,-1]\n",
        "(X_train, X_test,\n",
        "y_train, y_test) = train_test_split(X, y, random_state=0, shuffle=True)"
      ],
      "metadata": {
        "id": "_VsUaB4Xtmsz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision trees like Logical regression models stand the chance of overfitting data, thus the data is trained on 5 shuffled sets of the data."
      ],
      "metadata": {
        "id": "IZHttGg_vts7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier(min_samples_split=5)\n",
        "dt.fit(X_train, y_train)\n",
        "accuracy = dt.score(X_test, y_test)\n",
        "print(f\"test accuracy: {accuracy:0.3}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJWf6idUwHND",
        "outputId": "3a8edfb2-9d10-4629-a147-0bb4d1950e32"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test accuracy: 0.774\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the fact that decision trees are not accountable for when they stop unless a stop limit is specified or target values are singular, they continue endlessly exploring different paths. The term pruning refers to clamping down on the decision tree to render a simple and more precise model of the data with a more precise set of rules. "
      ],
      "metadata": {
        "id": "1M3OwoAZxru3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = dt.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities #alpha is an indicator of how good/complete a tree is, since pruning makes a tree complete, impurity is an indicator of chaos at a node/region"
      ],
      "metadata": {
        "id": "XyzIB3xJyhFW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_pruning = list()\n",
        "for ccp_alpha in ccp_alphas:\n",
        "  if ccp_alpha > 0:\n",
        "    dt = DecisionTreeClassifier(random_state=0,ccp_alpha=ccp_alpha)\n",
        "    dt.fit(X_train, y_train)\n",
        "    best_pruning.append([ccp_alpha, dt.score(X_test,y_test)])\n",
        "best_pruning = sorted(best_pruning,key=lambda x:x[1], reverse=True)\n",
        "best_ccp_alpha = best_pruning[0][0]\n",
        "dt = DecisionTreeClassifier(random_state=0,ccp_alpha=best_ccp_alpha)\n",
        "dt.fit(X_train, y_train)\n",
        "accuracy = dt.score(X_test, y_test)\n",
        "print(f\"test accuracy: {accuracy:0.3}\") #accuracy has improved"
      ],
      "metadata": {
        "id": "GJtFKBfezDU9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}